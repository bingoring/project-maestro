# RAG ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ¯ RAG (Retrieval-Augmented Generation) ê°œìš”

RAGëŠ” ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. Project Maestroì—ì„œëŠ” ê²Œì„ ê°œë°œ ì§€ì‹, Unity ë¬¸ì„œ, ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë“±ì„ RAGë¡œ í™œìš©í•©ë‹ˆë‹¤.

## ğŸ—ï¸ RAG ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

```mermaid
graph TD
    A[ë¬¸ì„œ ë¡œë”] --> B[í…ìŠ¤íŠ¸ ë¶„í• ]
    B --> C[ì„ë² ë”© ìƒì„±]
    C --> D[ë²¡í„° ìŠ¤í† ì–´]
    D --> E[ê²€ìƒ‰ê¸°]
    E --> F[ì»¨í…ìŠ¤íŠ¸ ì••ì¶•]
    F --> G[LLM ìƒì„±]
    
    H[ì‚¬ìš©ì ì¿¼ë¦¬] --> E
    I[ë©”íƒ€ë°ì´í„°] --> D
    J[í”¼ë“œë°±] --> K[ì„±ëŠ¥ ê°œì„ ]
    G --> K
```

### 2. ë°ì´í„° íŒŒì´í”„ë¼ì¸

#### ë¬¸ì„œ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
```python
class DocumentPipeline:
    """ê²Œì„ ê°œë°œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.loaders = {
            "unity_docs": UnityDocumentLoader(),
            "game_design": GameDesignLoader(),
            "code_examples": CodeExampleLoader(),
            "best_practices": BestPracticeLoader()
        }
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", " "]
        )
    
    async def process_documents(self, source_type: str, path: str):
        """ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬"""
        loader = self.loaders[source_type]
        raw_documents = await loader.load(path)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for doc in raw_documents:
            doc.metadata.update({
                "source_type": source_type,
                "processed_at": datetime.now().isoformat(),
                "chunk_strategy": "recursive"
            })
        
        # ë¬¸ì„œ ë¶„í• 
        chunks = self.text_splitter.split_documents(raw_documents)
        
        return chunks
```

#### ì„ë² ë”© ì „ëµ
```python
class HybridEmbeddingStrategy:
    """ë¹„ìš© íš¨ìœ¨ì ì¸ í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”©"""
    
    def __init__(self):
        # ê³ í’ˆì§ˆ ì„ë² ë”© (ì¤‘ìš” ë¬¸ì„œìš©)
        self.openai_embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            dimensions=1536
        )
        
        # ë¡œì»¬ ì„ë² ë”© (ì¼ë°˜ ë¬¸ì„œìš©)
        self.local_embeddings = SentenceTransformerEmbeddings(
            model_name="all-MiniLM-L6-v2"
        )
        
        # ì½”ë“œ íŠ¹í™” ì„ë² ë”©
        self.code_embeddings = SentenceTransformerEmbeddings(
            model_name="microsoft/CodeBERT-base"
        )
    
    async def embed_document(self, doc: Document) -> List[float]:
        """ë¬¸ì„œ íƒ€ì…ë³„ ìµœì  ì„ë² ë”© ì„ íƒ"""
        if doc.metadata.get("importance") == "high":
            return await self.openai_embeddings.aembed_query(doc.page_content)
        elif doc.metadata.get("content_type") == "code":
            return await self.code_embeddings.aembed_query(doc.page_content)
        else:
            return await self.local_embeddings.aembed_query(doc.page_content)
```

### 3. ë²¡í„° ìŠ¤í† ì–´ ìµœì í™”

#### Chroma DB ì„¤ì •
```python
class OptimizedVectorStore:
    """ì„±ëŠ¥ ìµœì í™”ëœ ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(self, persist_directory: str = "data/vectorstore"):
        self.persist_directory = persist_directory
        
        # HNSW ì¸ë±ìŠ¤ ìµœì í™” ì„¤ì •
        self.collection_metadata = {
            "hnsw:space": "cosine",           # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            "hnsw:construction_ef": 400,     # ì¸ë±ìŠ¤ êµ¬ì¶• í’ˆì§ˆ
            "hnsw:M": 16,                    # ì—°ê²° ìˆ˜ (ë©”ëª¨ë¦¬ vs ì†ë„)
            "hnsw:search_ef": 100            # ê²€ìƒ‰ í’ˆì§ˆ
        }
        
    async def initialize(self, embeddings: Embeddings):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        self.vector_store = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=embeddings,
            collection_metadata=self.collection_metadata
        )
        
        # ì»¬ë ‰ì…˜ë³„ ë¶„ë¦¬ (ê²Œì„ íƒ€ì…ë³„)
        self.collections = {
            "unity_docs": self._create_collection("unity_docs"),
            "game_patterns": self._create_collection("game_patterns"),
            "code_examples": self._create_collection("code_examples")
        }
    
    def _create_collection(self, collection_name: str):
        """íŠ¹í™”ëœ ì»¬ë ‰ì…˜ ìƒì„±"""
        return Chroma(
            collection_name=collection_name,
            persist_directory=f"{self.persist_directory}/{collection_name}",
            embedding_function=self.embeddings,
            collection_metadata=self.collection_metadata
        )
```

#### ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ
```python
class DocumentMetadata(BaseModel):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ"""
    
    # ê¸°ë³¸ ì •ë³´
    source_type: str          # unity_docs, game_design, code_example
    content_type: str         # text, code, image, audio
    language: str = "ko"      # ì–¸ì–´ ì„¤ì •
    
    # ê²Œì„ ê°œë°œ íŠ¹í™”
    game_genre: Optional[str] = None      # platformer, rpg, puzzle
    unity_version: Optional[str] = None   # Unity ë²„ì „ í˜¸í™˜ì„±
    platform: Optional[str] = None       # mobile, desktop, web
    
    # í’ˆì§ˆ ì§€í‘œ
    importance: str = "medium"            # high, medium, low
    confidence_score: float = 0.8         # ë¬¸ì„œ ì‹ ë¢°ë„
    last_updated: datetime                # ìµœì¢… ì—…ë°ì´íŠ¸
    
    # ì ‘ê·¼ ì œì–´
    access_level: str = "public"          # public, internal, restricted
    tags: List[str] = []                  # ê²€ìƒ‰ íƒœê·¸
```

## ğŸ” ê³ ê¸‰ ê²€ìƒ‰ ì „ëµ

### 1. Multi-Query RAG
```python
class MultiQueryRAG:
    """ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ í†µí•œ í–¥ìƒëœ ê²€ìƒ‰"""
    
    def __init__(self, llm: BaseLanguageModel, vector_store: VectorStore):
        self.llm = llm
        self.vector_store = vector_store
        
        # ì¿¼ë¦¬ ìƒì„± ì²´ì¸
        self.query_generation_chain = (
            ChatPromptTemplate.from_template(
                """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
                
                ì›ë³¸ ì§ˆë¬¸: {question}
                
                ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ (ê° ì¤„ì— í•˜ë‚˜ì”©):"""
            )
            | self.llm
            | StrOutputParser()
            | RunnableLambda(lambda x: [q.strip() for q in x.split("\n") if q.strip()])
        )
    
    async def multi_query_retrieve(self, question: str, k: int = 5) -> List[Document]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ í†µí•œ ê²€ìƒ‰"""
        # ë‹¤ì–‘í•œ ì¿¼ë¦¬ ìƒì„±
        queries = await self.query_generation_chain.ainvoke({"question": question})
        
        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        all_docs = []
        for query in queries:
            docs = await self.vector_store.asimilarity_search(query, k=k//len(queries)+1)
            all_docs.extend(docs)
        
        # ì¤‘ë³µ ì œê±° ë° ì¬ìˆœìœ„í™”
        unique_docs = self._remove_duplicates(all_docs)
        reranked_docs = await self._rerank_documents(question, unique_docs)
        
        return reranked_docs[:k]
    
    def _remove_duplicates(self, docs: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique_docs = []
        
        for doc in docs:
            # ë‚´ìš© í•´ì‹œë¡œ ì¤‘ë³µ íŒì •
            content_hash = hash(doc.page_content)
            if content_hash not in seen:
                seen.add(content_hash)
                unique_docs.append(doc)
                
        return unique_docs
    
    async def _rerank_documents(self, query: str, docs: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ì¬ìˆœìœ„í™”"""
        # ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜ ì¬ìˆœìœ„í™” (ì‹¤ì œë¡œëŠ” cross-encoder ì‚¬ìš© ê¶Œì¥)
        scores = []
        for doc in docs:
            relevance = await self._calculate_relevance(query, doc.page_content)
            scores.append((doc, relevance))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)
        return [doc for doc, score in ranked_docs]
```

### 2. HyDE (Hypothetical Document Embedding)
```python
class HyDERAG:
    """ê°€ìƒ ë¬¸ì„œ ì„ë² ë”©ì„ í†µí•œ ê²€ìƒ‰ í–¥ìƒ"""
    
    def __init__(self, llm: BaseLanguageModel, vector_store: VectorStore):
        self.llm = llm
        self.vector_store = vector_store
        
        # HyDE í”„ë¡¬í”„íŠ¸
        self.hyde_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
            
            ì§ˆë¬¸: {question}
            
            ë‹µë³€:"""
        )
        
        # HyDE ì²´ì¸
        self.hyde_chain = (
            self.hyde_prompt
            | self.llm
            | StrOutputParser()
        )
    
    async def hyde_retrieve(self, question: str, k: int = 5) -> List[Document]:
        """HyDE ë°©ì‹ ê²€ìƒ‰"""
        # 1ë‹¨ê³„: ê°€ìƒ ë‹µë³€ ìƒì„±
        hypothetical_answer = await self.hyde_chain.ainvoke({"question": question})
        
        # 2ë‹¨ê³„: ê°€ìƒ ë‹µë³€ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
        docs = await self.vector_store.asimilarity_search(hypothetical_answer, k=k*2)
        
        # 3ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ìœ¼ë¡œ ì¬í•„í„°ë§
        filtered_docs = await self._filter_by_relevance(question, docs, k)
        
        return filtered_docs
    
    async def _filter_by_relevance(
        self, 
        original_question: str, 
        docs: List[Document], 
        k: int
    ) -> List[Document]:
        """ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ìœ¼ë¡œ í•„í„°ë§"""
        # ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ìœ ì‚¬ë„ ì¬ê³„ì‚°
        original_results = await self.vector_store.asimilarity_search(original_question, k=k)
        original_content = {doc.page_content for doc in original_results}
        
        # êµì§‘í•© ìš°ì„ , ê·¸ ë‹¤ìŒ HyDE ê²°ê³¼
        prioritized_docs = []
        seen_content = set()
        
        # 1ìˆœìœ„: ë‘ ë°©ë²• ëª¨ë‘ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ
        for doc in docs:
            if doc.page_content in original_content and doc.page_content not in seen_content:
                prioritized_docs.append(doc)
                seen_content.add(doc.page_content)
        
        # 2ìˆœìœ„: HyDEì—ì„œë§Œ ê²€ìƒ‰ëœ ë¬¸ì„œ
        for doc in docs:
            if doc.page_content not in seen_content and len(prioritized_docs) < k:
                prioritized_docs.append(doc)
                seen_content.add(doc.page_content)
        
        return prioritized_docs[:k]
```

### 3. Contextual Compression
```python
class ContextualCompressionRAG:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ì„ í†µí•œ ì •ë³´ ì •ì œ"""
    
    def __init__(self, llm: BaseLanguageModel, retriever: BaseRetriever):
        self.llm = llm
        self.retriever = retriever
        
        # ì••ì¶• í”„ë¡¬í”„íŠ¸
        self.compression_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”:
            
            ì§ˆë¬¸: {question}
            
            ì»¨í…ìŠ¤íŠ¸:
            {context}
            
            í•µì‹¬ ì •ë³´:
            - ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ í¬í•¨
            - êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì½”ë“œê°€ ìˆë‹¤ë©´ í¬í•¨
            - ë¶ˆí•„ìš”í•œ ì¼ë°˜ë¡ ì€ ì œì™¸
            
            ì¶”ì¶œëœ ì •ë³´:"""
        )
        
        # ì••ì¶• ì²´ì¸
        self.compression_chain = (
            self.compression_prompt
            | self.llm
            | StrOutputParser()
        )
    
    async def compressed_retrieve(self, question: str, k: int = 10) -> List[Document]:
        """ì••ì¶•ëœ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜"""
        # 1ë‹¨ê³„: ê¸°ë³¸ ê²€ìƒ‰ (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
        raw_docs = await self.retriever.aget_relevant_documents(question)
        
        # 2ë‹¨ê³„: ê° ë¬¸ì„œë³„ ì••ì¶•
        compressed_docs = []
        for doc in raw_docs[:k*2]:  # 2ë°° ê²€ìƒ‰ í›„ ì••ì¶•
            compressed_content = await self.compression_chain.ainvoke({
                "question": question,
                "context": doc.page_content
            })
            
            # ì˜ë¯¸ ìˆëŠ” ì••ì¶•ì´ ì´ë£¨ì–´ì§„ ê²½ìš°ë§Œ í¬í•¨
            if len(compressed_content.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                compressed_doc = Document(
                    page_content=compressed_content,
                    metadata={
                        **doc.metadata,
                        "compression_applied": True,
                        "original_length": len(doc.page_content),
                        "compressed_length": len(compressed_content)
                    }
                )
                compressed_docs.append(compressed_doc)
        
        return compressed_docs[:k]
```

## ğŸ® ê²Œì„ ê°œë°œ íŠ¹í™” RAG

### 1. Unity ë¬¸ì„œ RAG ì‹œìŠ¤í…œ
```python
class UnityRAGSystem:
    """Unity ê°œë°œ íŠ¹í™” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.unity_version = "2023.2.0f1"
        
        # Unity ë¬¸ì„œ êµ¬ì¡°
        self.document_categories = {
            "api_reference": "Unity API ë ˆí¼ëŸ°ìŠ¤",
            "tutorials": "Unity íŠœí† ë¦¬ì–¼",
            "best_practices": "Unity ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤",
            "performance": "ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ",
            "platform_specific": "í”Œë«í¼ë³„ ê°œë°œ ê°€ì´ë“œ"
        }
        
        # ê²Œì„ ì¥ë¥´ë³„ íŒ¨í„´
        self.genre_patterns = {
            "platformer": {
                "movement_patterns": "í”Œë«í¬ë¨¸ ì´ë™ íŒ¨í„´",
                "physics_setup": "ë¬¼ë¦¬ ì„¤ì •",
                "level_design": "ë ˆë²¨ ë””ìì¸ ì›ì¹™"
            },
            "rpg": {
                "inventory_system": "ì¸ë²¤í† ë¦¬ ì‹œìŠ¤í…œ",
                "character_progression": "ìºë¦­í„° ì„±ì¥",
                "quest_system": "í€˜ìŠ¤íŠ¸ ì‹œìŠ¤í…œ"
            }
        }
    
    async def create_unity_rag_chain(self, target_genre: str = None):
        """Unity íŠ¹í™” RAG ì²´ì¸ ìƒì„±"""
        
        # ì¥ë¥´ë³„ í•„í„°ë§ retriever
        genre_retriever = self.vector_store.as_retriever(
            search_kwargs={
                "k": 10,
                "filter": {"genre": target_genre} if target_genre else {}
            }
        )
        
        # Unity íŠ¹í™” í”„ë¡¬í”„íŠ¸
        unity_prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ Unity ê²Œì„ ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ Unity ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

Unity ë¬¸ì„œ:
{context}

ê°œë°œì ì§ˆë¬¸: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. Unity {unity_version} ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì¸ ì½”ë“œ ì˜ˆì‹œ í¬í•¨
3. ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­ ì–¸ê¸‰
4. í”Œë«í¼ë³„ ì°¨ì´ì  ì„¤ëª…
5. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ê¶Œì¥ì‚¬í•­

ë‹µë³€:"""
        )
        
        # Unity RAG ì²´ì¸
        unity_chain = (
            RunnableParallel({
                "context": genre_retriever | self._format_unity_docs,
                "question": RunnablePassthrough(),
                "unity_version": RunnableLambda(lambda _: self.unity_version)
            })
            | unity_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return unity_chain
    
    def _format_unity_docs(self, docs: List[Document]) -> str:
        """Unity ë¬¸ì„œ í¬ë§·íŒ…"""
        formatted_sections = []
        
        for doc in docs:
            section = f"[{doc.metadata.get('category', 'General')}]"
            if 'api_class' in doc.metadata:
                section += f" API: {doc.metadata['api_class']}"
            if 'unity_version' in doc.metadata:
                section += f" (Unity {doc.metadata['unity_version']})"
            
            section += f"\n{doc.page_content}\n"
            formatted_sections.append(section)
        
        return "\n---\n".join(formatted_sections)
```

### 2. ê²Œì„ ë””ìì¸ íŒ¨í„´ RAG
```python
class GameDesignPatternRAG:
    """ê²Œì„ ë””ìì¸ íŒ¨í„´ íŠ¹í™” RAG"""
    
    def __init__(self, llm: BaseLanguageModel):
        self.llm = llm
        
        # ë””ìì¸ íŒ¨í„´ ë¶„ë¥˜
        self.pattern_categories = {
            "behavioral": "í–‰ë™ íŒ¨í„´ (í”Œë ˆì´ì–´ í–‰ë™, AI)",
            "structural": "êµ¬ì¡° íŒ¨í„´ (ë°ì´í„°, ì•„í‚¤í…ì²˜)",
            "gameplay": "ê²Œì„í”Œë ˆì´ íŒ¨í„´ (ë£¨í”„, ì§„í–‰)",
            "ui_ux": "UI/UX íŒ¨í„´ (ì¸í„°í˜ì´ìŠ¤, í”¼ë“œë°±)"
        }
    
    async def create_pattern_rag_chain(self):
        """ê²Œì„ ë””ìì¸ íŒ¨í„´ RAG ì²´ì¸"""
        
        # íŒ¨í„´ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        pattern_prompt = ChatPromptTemplate.from_template(
            """ê²Œì„ ë””ìì¸ íŒ¨í„´ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ íŒ¨í„´ì„ ì œì•ˆí•˜ì„¸ìš”.

ê´€ë ¨ íŒ¨í„´ ë¬¸ì„œ:
{context}

ê²Œì„ ìš”êµ¬ì‚¬í•­: {question}

ë¶„ì„ ê²°ê³¼:
1. ì ìš© ê°€ëŠ¥í•œ íŒ¨í„´: 
2. êµ¬í˜„ ë°©ë²•:
3. ì˜ˆìƒ ë¬¸ì œì :
4. ëŒ€ì•ˆ íŒ¨í„´:
5. ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­:

ìƒì„¸ ë‹µë³€:"""
        )
        
        # íŒ¨í„´ë³„ retriever
        pattern_retriever = self.vector_store.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={
                "k": 8,
                "lambda_mult": 0.7  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê· í˜•
            }
        )
        
        # íŒ¨í„´ RAG ì²´ì¸
        pattern_chain = (
            RunnableParallel({
                "context": pattern_retriever | self._format_patterns,
                "question": RunnablePassthrough()
            })
            | pattern_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return pattern_chain
    
    def _format_patterns(self, docs: List[Document]) -> str:
        """íŒ¨í„´ ë¬¸ì„œ í¬ë§·íŒ…"""
        sections = []
        
        for doc in docs:
            pattern_name = doc.metadata.get("pattern_name", "Unknown Pattern")
            category = doc.metadata.get("category", "General")
            complexity = doc.metadata.get("complexity", "Medium")
            
            section = f"""
== {pattern_name} ({category}) ==
ë³µì¡ë„: {complexity}
ë‚´ìš©: {doc.page_content}
"""
            sections.append(section)
        
        return "\n".join(sections)
```

### 3. ì½”ë“œ ì˜ˆì‹œ RAG
```python
class CodeExampleRAG:
    """Unity C# ì½”ë“œ ì˜ˆì‹œ íŠ¹í™” RAG"""
    
    def __init__(self, llm: BaseLanguageModel):
        self.llm = llm
        
        # ì½”ë“œ ë¶„ì„ ì²´ì¸
        self.code_analysis_chain = (
            ChatPromptTemplate.from_template(
                """Unity C# ì½”ë“œ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì½”ë“œ ì˜ˆì‹œë“¤ì„ ë¶„ì„í•˜ê³  ìš”ì²­ì— ë§ëŠ” ì†”ë£¨ì…˜ì„ ì œê³µí•˜ì„¸ìš”.

ê´€ë ¨ ì½”ë“œ ì˜ˆì‹œë“¤:
{context}

ê°œë°œ ìš”êµ¬ì‚¬í•­: {question}

ì†”ë£¨ì…˜:
1. ì¶”ì²œ ì ‘ê·¼ë²•:
2. í•µì‹¬ ì½”ë“œ:
```csharp
// ì—¬ê¸°ì— Unity C# ì½”ë“œ ì‘ì„±
```
3. ì„±ëŠ¥ ìµœì í™”:
4. ì£¼ì˜ì‚¬í•­:
5. í…ŒìŠ¤íŠ¸ ë°©ë²•:

ìƒì„¸ ì„¤ëª…:"""
            )
            | self.llm
            | StrOutputParser()
        )
    
    async def get_code_solution(self, requirement: str) -> str:
        """ì½”ë“œ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ì†”ë£¨ì…˜ ì œê³µ"""
        
        # ì½”ë“œ íŠ¹í™” ê²€ìƒ‰
        code_docs = await self._search_code_examples(requirement)
        
        # ì½”ë“œ ì†”ë£¨ì…˜ ìƒì„±
        solution = await self.code_analysis_chain.ainvoke({
            "context": self._format_code_examples(code_docs),
            "question": requirement
        })
        
        return solution
    
    async def _search_code_examples(self, requirement: str) -> List[Document]:
        """ì½”ë“œ ì˜ˆì‹œ ê²€ìƒ‰"""
        # ì½”ë“œ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ
        code_keywords = await self._extract_code_keywords(requirement)
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
        all_examples = []
        for keyword in code_keywords:
            examples = await self.vector_store.asimilarity_search(
                keyword,
                k=3,
                filter={"content_type": "code"}
            )
            all_examples.extend(examples)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ì •ë ¬
        unique_examples = self._deduplicate_code_examples(all_examples)
        return unique_examples[:5]
    
    def _format_code_examples(self, docs: List[Document]) -> str:
        """ì½”ë“œ ì˜ˆì‹œ í¬ë§·íŒ…"""
        formatted = []
        
        for i, doc in enumerate(docs, 1):
            example_title = doc.metadata.get("title", f"ì˜ˆì‹œ {i}")
            unity_version = doc.metadata.get("unity_version", "N/A")
            
            formatted.append(f"""
=== ì˜ˆì‹œ {i}: {example_title} (Unity {unity_version}) ===
{doc.page_content}
""")
        
        return "\n".join(formatted)
```

## ğŸ”§ í†µí•© RAG ì‹œìŠ¤í…œ

### 1. ë§ˆìŠ¤í„° RAG ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
```python
class MaestroRAGSystem:
    """Project Maestro í†µí•© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # íŠ¹í™”ëœ RAG ì‹œìŠ¤í…œë“¤
        self.unity_rag = UnityRAGSystem()
        self.pattern_rag = GameDesignPatternRAG()
        self.code_rag = CodeExampleRAG()
        
        # ì¿¼ë¦¬ ë¶„ë¥˜ê¸°
        self.query_classifier = self._create_query_classifier()
    
    def _create_query_classifier(self):
        """ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜ê¸°"""
        classification_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ì§ˆë¬¸ì„ ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

ì¹´í…Œê³ ë¦¬:
- unity_api: Unity API ì‚¬ìš©ë²•
- game_pattern: ê²Œì„ ë””ìì¸ íŒ¨í„´
- code_example: ì½”ë“œ êµ¬í˜„ ì˜ˆì‹œ
- general: ì¼ë°˜ì ì¸ ê²Œì„ ê°œë°œ

ì§ˆë¬¸: {question}

ì¹´í…Œê³ ë¦¬: """
        )
        
        return (
            classification_prompt
            | self.llm
            | StrOutputParser()
            | RunnableLambda(lambda x: x.strip().lower())
        )
    
    async def intelligent_retrieve(self, question: str) -> str:
        """ì§€ëŠ¥ì  RAG ê²€ìƒ‰ ë° ì‘ë‹µ"""
        
        # 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ë¥˜
        category = await self.query_classifier.ainvoke({"question": question})
        
        # 2ë‹¨ê³„: íŠ¹í™”ëœ RAG ì‹œìŠ¤í…œ ì„ íƒ
        if category == "unity_api":
            return await self.unity_rag.query(question)
        elif category == "game_pattern":
            return await self.pattern_rag.query(question)
        elif category == "code_example":
            return await self.code_rag.get_code_solution(question)
        else:
            # í†µí•© ê²€ìƒ‰
            return await self._general_rag_query(question)
    
    async def _general_rag_query(self, question: str) -> str:
        """í†µí•© RAG ì¿¼ë¦¬"""
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
        unity_docs = await self.unity_rag.search(question, k=3)
        pattern_docs = await self.pattern_rag.search(question, k=3)
        code_docs = await self.code_rag.search(question, k=3)
        
        # í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        all_docs = unity_docs + pattern_docs + code_docs
        context = self._format_mixed_context(all_docs)
        
        # í†µí•© ì‘ë‹µ ìƒì„±
        general_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ì–‘í•œ ê²Œì„ ê°œë°œ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:

í†µí•© ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ì¢…í•© ë‹µë³€:"""
        )
        
        general_chain = (
            RunnableParallel({
                "context": RunnableLambda(lambda _: context),
                "question": RunnablePassthrough()
            })
            | general_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return await general_chain.ainvoke({"question": question})
```

### 2. ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ
```python
class AdaptiveRAGSystem:
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì ì‘í˜• RAG"""
    
    def __init__(self):
        self.feedback_store = FeedbackStore()
        self.learning_scheduler = LearningScheduler()
    
    async def query_with_feedback_loop(self, question: str, user_id: str) -> Dict:
        """í”¼ë“œë°± ë£¨í”„ê°€ í¬í•¨ëœ RAG ì¿¼ë¦¬"""
        
        # 1ë‹¨ê³„: ê¸°ë³¸ RAG ì‘ë‹µ
        response = await self.rag_chain.ainvoke({"question": question})
        
        # 2ë‹¨ê³„: ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡
        satisfaction_score = await self._predict_satisfaction(question, response)
        
        # 3ë‹¨ê³„: ë‚®ì€ ì ìˆ˜ ì‹œ ëŒ€ì•ˆ ê²€ìƒ‰
        if satisfaction_score < 0.7:
            alternative_response = await self._generate_alternative(question)
            response = alternative_response
        
        return {
            "response": response,
            "confidence": satisfaction_score,
            "query_id": str(uuid.uuid4()),
            "user_id": user_id
        }
    
    async def record_feedback(self, query_id: str, rating: float, comments: str = ""):
        """ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡"""
        await self.feedback_store.record({
            "query_id": query_id,
            "rating": rating,
            "comments": comments,
            "timestamp": datetime.now()
        })
        
        # ì£¼ê¸°ì  ëª¨ë¸ ì¬í•™ìŠµ íŠ¸ë¦¬ê±°
        await self.learning_scheduler.schedule_retraining()
    
    async def _predict_satisfaction(self, question: str, response: str) -> float:
        """ì‘ë‹µ ë§Œì¡±ë„ ì˜ˆì¸¡"""
        satisfaction_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ì§ˆë¬¸-ë‹µë³€ ìŒì˜ í’ˆì§ˆì„ 0-1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}
ë‹µë³€: {response}

í‰ê°€ ê¸°ì¤€:
- ì •í™•ì„±: ê¸°ìˆ ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ê°€?
- ì™„ì „ì„±: ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí–ˆëŠ”ê°€?
- ì‹¤ìš©ì„±: ì‹¤ì œë¡œ ì ìš© ê°€ëŠ¥í•œê°€?
- ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?

ì ìˆ˜ (0.0-1.0): """
        )
        
        score_str = await (satisfaction_prompt | self.llm | StrOutputParser()).ainvoke({
            "question": question,
            "response": response
        })
        
        try:
            return float(score_str.strip())
        except ValueError:
            return 0.5  # ê¸°ë³¸ê°’
```

## ğŸ“Š RAG ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹± ì „ëµ
```python
import redis
from functools import lru_cache

class RAGCache:
    """RAG ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            decode_responses=True
        )
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼)
        self.memory_cache = {}
        self.max_memory_items = 1000
    
    @lru_cache(maxsize=500)
    def _embed_query_cached(self, query: str) -> str:
        """ì„ë² ë”© ê²°ê³¼ ë©”ëª¨ë¦¬ ìºì‹±"""
        return self.embeddings.embed_query(query)
    
    async def get_cached_response(self, query: str) -> Optional[str]:
        """ìºì‹œëœ ì‘ë‹µ ê²€ìƒ‰"""
        query_hash = hashlib.sha256(query.encode()).hexdigest()
        
        # 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if query_hash in self.memory_cache:
            return self.memory_cache[query_hash]
        
        # 2ë‹¨ê³„: Redis ìºì‹œ í™•ì¸
        cached_response = await self.redis_client.get(f"rag:{query_hash}")
        if cached_response:
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
            if len(self.memory_cache) < self.max_memory_items:
                self.memory_cache[query_hash] = cached_response
            return cached_response
        
        return None
    
    async def cache_response(self, query: str, response: str, ttl: int = 3600):
        """ì‘ë‹µ ìºì‹±"""
        query_hash = hashlib.sha256(query.encode()).hexdigest()
        
        # Redisì— TTLê³¼ í•¨ê»˜ ì €ì¥
        await self.redis_client.setex(f"rag:{query_hash}", ttl, response)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
        if len(self.memory_cache) < self.max_memory_items:
            self.memory_cache[query_hash] = response
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
class BatchRAGProcessor:
    """ë°°ì¹˜ RAG ì²˜ë¦¬ ìµœì í™”"""
    
    async def batch_query_processing(self, questions: List[str], batch_size: int = 10):
        """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        
        # ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬
        embeddings = await self.embeddings.aembed_documents(questions)
        
        # ë²¡í„° ê²€ìƒ‰ ë°°ì¹˜ ì²˜ë¦¬
        all_docs = []
        for embedding in embeddings:
            docs = await self.vector_store.asimilarity_search_by_vector(
                embedding, k=5
            )
            all_docs.append(docs)
        
        # LLM ë°°ì¹˜ ì²˜ë¦¬
        contexts = [self._format_docs(docs) for docs in all_docs]
        prompts = [self._create_prompt(q, c) for q, c in zip(questions, contexts)]
        
        responses = await self.llm.abatch(prompts, config={"max_concurrency": batch_size})
        
        return [response.content for response in responses]
    
    async def parallel_rag_chains(self, question: str) -> Dict[str, str]:
        """ì—¬ëŸ¬ RAG ì²´ì¸ ë³‘ë ¬ ì‹¤í–‰"""
        
        # ë‹¤ì–‘í•œ ê´€ì ì˜ ì²´ì¸ë“¤
        chains = {
            "technical": self.technical_rag_chain,
            "creative": self.creative_rag_chain,
            "practical": self.practical_rag_chain
        }
        
        # ë³‘ë ¬ ì‹¤í–‰
        tasks = {
            name: chain.ainvoke({"question": question})
            for name, chain in chains.items()
        }
        
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)
        
        return {
            name: result if not isinstance(result, Exception) else f"Error: {result}"
            for name, result in zip(chains.keys(), results)
        }
```

## ğŸ“ ë©´ì ‘ í•µì‹¬ í¬ì¸íŠ¸

### RAG ì‹œìŠ¤í…œ ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­

1. **ë°ì´í„° í’ˆì§ˆ**: ì†ŒìŠ¤ ë¬¸ì„œì˜ í’ˆì§ˆì´ RAG ì„±ëŠ¥ì„ ì¢Œìš°
2. **ì²­í‚¹ ì „ëµ**: ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• ë¡œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
3. **ì„ë² ë”© ì„ íƒ**: ë¹„ìš©, ì„±ëŠ¥, ì •í™•ë„ì˜ ê· í˜•
4. **ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜**: ìœ ì‚¬ë„, MMR, í•„í„°ë§ì˜ ì¡°í•©
5. **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ê³ ë ¤
6. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì‹œ ì¦‰ì‹œ ë°˜ì˜
7. **í”¼ë“œë°± ë£¨í”„**: ì‚¬ìš©ì ë§Œì¡±ë„ë¥¼ í†µí•œ ì§€ì† ê°œì„ 

### ì„±ëŠ¥ ë³‘ëª©ì§€ì  ë° í•´ê²°ì±…

1. **ì„ë² ë”© ìƒì„±**: ë¹„ìš©ì´ ë†’ì€ OpenAI API â†’ ë¡œì»¬ ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ
2. **ë²¡í„° ê²€ìƒ‰**: ëŒ€ìš©ëŸ‰ ë°ì´í„° â†’ ì¸ë±ìŠ¤ ìµœì í™”, ê³„ì¸µì  ê²€ìƒ‰
3. **LLM í˜¸ì¶œ**: ì§€ì—° ì‹œê°„ â†’ ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬
4. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: í° ë²¡í„° ìŠ¤í† ì–´ â†’ ì••ì¶•, í”„ë£¨ë‹
5. **ë™ì‹œ ìš”ì²­**: ë¦¬ì†ŒìŠ¤ ê²½í•© â†’ ì—°ê²° í’€, íì‰

ì´ ê°€ì´ë“œë¥¼ í†µí•´ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”ì§€ë‹ˆì–´ ë©´ì ‘ì—ì„œ RAG ë° ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.